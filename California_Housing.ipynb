{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# California Housing Price Prediction\n", "\n", "End-to-end regression project with preprocessing, model comparison, cross\u2011validation and hyperparameter tuning.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Setup\n", "\n", "Check Python & scikit\u2011learn versions and import core libraries."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import sys\n", "import numpy as np\n", "import pandas as pd\n", "\n", "assert sys.version_info >= (3, 7)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from packaging import version\n", "import sklearn\n", "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from pathlib import Path\n", "import tarfile\n", "import urllib.request\n", "\n", "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n", "\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n", "from sklearn.svm import SVR\n", "\n", "from xgboost import XGBRegressor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load the data\n", "\n", "We use the **California housing** dataset from Aur\u00e9lien G\u00e9ron's GitHub repo and cache it locally under `datasets/housing/`."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def load_housing_data():\n", "    tarball_path = Path(\"datasets/housing.tgz\")\n", "    if not tarball_path.is_file():\n", "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n", "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n", "        urllib.request.urlretrieve(url, tarball_path)\n", "    with tarfile.open(tarball_path) as housing_tarball:\n", "        housing_tarball.extractall(path=\"datasets\")\n", "    csv_path = Path(\"datasets/housing/housing.csv\")\n", "    return pd.read_csv(csv_path)\n", "\n", "df = load_housing_data()\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Quick data inspection"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df.info()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df.describe().T"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df['ocean_proximity'].value_counts()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["_ = df.hist(bins=50, figsize=(12, 8))\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Train\u2013test split\n", "\n", "We first separate features and target, then create train/test sets. Splitting **before** any preprocessing avoids data leakage."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X = df.drop(\"median_house_value\", axis=1)\n", "y = df[\"median_house_value\"].copy()\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.2, random_state=42\n", ")\n", "\n", "X_train.shape, X_test.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Preprocessing pipeline\n", "\n", "We apply different transformations to numeric and categorical columns using a `ColumnTransformer` and `Pipeline`.\n", "\n", "- **Numeric features**: median imputation + standard scaling\n", "- **Categorical features**: most\u2011frequent imputation + one\u2011hot encoding"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n", "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n", "\n", "cat_attribs = [\"ocean_proximity\"]"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["num_pipeline = Pipeline([\n", "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n", "    (\"scaler\", StandardScaler()),\n", "])\n", "\n", "cat_pipeline = Pipeline([\n", "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n", "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n", "])\n", "\n", "preprocessing = ColumnTransformer(\n", "    [\n", "        (\"num\", num_pipeline, num_attribs),\n", "        (\"cat\", cat_pipeline, cat_attribs),\n", "    ],\n", "    remainder=\"drop\",\n", ")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Sanity\u2011check: transformed matrices contain no NaNs\n", "X_train_prepared = preprocessing.fit_transform(X_train)\n", "X_test_prepared = preprocessing.transform(X_test)\n", "\n", "print(\"NaNs in X_train_prepared:\", np.isnan(X_train_prepared).sum())\n", "print(\"NaNs in X_test_prepared:\", np.isnan(X_test_prepared).sum())\n", "print(\"Prepared shapes:\", X_train_prepared.shape, X_test_prepared.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Helper functions\n", "\n", "Convenience functions for model evaluation and comparison."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def regression_metrics(y_true, y_pred):\n", "    mse = mean_squared_error(y_true, y_pred)\n", "    rmse = np.sqrt(mse)\n", "    mae = mean_absolute_error(y_true, y_pred)\n", "    r2 = r2_score(y_true, y_pred)\n", "    return rmse, mae, r2"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def cross_validate_models(models, preprocessing, X, y, cv=5):\n", "    \"\"\"Run cross\u2011validation for multiple models.\n", "\n", "    Returns a DataFrame with mean CV scores.\n", "    \"\"\"\n", "    rows = []\n", "\n", "    for name, model in models.items():\n", "        print(f\"Running CV for: {name}\")\n", "\n", "        pipe = Pipeline([\n", "            (\"preprocessing\", preprocessing),\n", "            (\"model\", model),\n", "        ])\n", "\n", "        r2_scores = cross_val_score(\n", "            pipe, X, y,\n", "            cv=cv,\n", "            scoring=\"r2\",\n", "            n_jobs=-1,\n", "        )\n", "\n", "        rmse_scores = -cross_val_score(\n", "            pipe, X, y,\n", "            cv=cv,\n", "            scoring=\"neg_root_mean_squared_error\",\n", "            n_jobs=-1,\n", "        )\n", "\n", "        mae_scores = -cross_val_score(\n", "            pipe, X, y,\n", "            cv=cv,\n", "            scoring=\"neg_mean_absolute_error\",\n", "            n_jobs=-1,\n", "        )\n", "\n", "        rows.append([\n", "            name,\n", "            r2_scores.mean(),\n", "            rmse_scores.mean(),\n", "            mae_scores.mean(),\n", "            r2_scores.mean() * 100,\n", "        ])\n", "\n", "    results = pd.DataFrame(\n", "        rows,\n", "        columns=[\"Model\", \"R2_mean\", \"RMSE_mean\", \"MAE_mean\", \"Performance_%\"],\n", "    )\n", "    return results.sort_values(by=\"R2_mean\", ascending=False).reset_index(drop=True)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def grid_search_all_models(models, param_grids, preprocessing, X_train, y_train, cv=3):\n", "    \"\"\"Run GridSearchCV for each model and return a summary DataFrame and dict of best estimators.\"\"\"\n", "    results = []\n", "    best_models = {}\n", "\n", "    for name, model in models.items():\n", "        print(f\"\\nGrid search for: {name}\")\n", "\n", "        pipe = Pipeline([\n", "            (\"preprocessing\", preprocessing),\n", "            (\"model\", model),\n", "        ])\n", "\n", "        grid = GridSearchCV(\n", "            pipe,\n", "            param_grid=param_grids[name],\n", "            cv=cv,\n", "            scoring=\"r2\",\n", "            n_jobs=-1,\n", "            verbose=2,\n", "        )\n", "        grid.fit(X_train, y_train)\n", "\n", "        best_models[name] = grid.best_estimator_\n", "        best_r2 = grid.best_score_\n", "        results.append([\n", "            name,\n", "            best_r2,\n", "            best_r2 * 100,\n", "            grid.best_params_,\n", "        ])\n", "\n", "    summary = pd.DataFrame(\n", "        results,\n", "        columns=[\"Model\", \"Best_CV_R2\", \"Performance_%\", \"Best_params\"],\n", "    )\n", "    return summary.sort_values(by=\"Best_CV_R2\", ascending=False).reset_index(drop=True), best_models"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def evaluate_best_models(best_models, X_test, y_test):\n", "    rows = []\n", "    for name, model in best_models.items():\n", "        print(f\"Evaluating on test set: {name}\")\n", "        y_pred = model.predict(X_test)\n", "        rmse, mae, r2 = regression_metrics(y_test, y_pred)\n", "        rows.append([name, rmse, mae, r2, r2 * 100])\n", "    return pd.DataFrame(\n", "        rows,\n", "        columns=[\"Model\", \"RMSE\", \"MAE\", \"R2\", \"Performance_%\"],\n", "    ).sort_values(by=\"R2\", ascending=False).reset_index(drop=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Model zoo\n", "\n", "We compare a set of tree\u2011based and kernel models that are well\u2011suited for tabular regression."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["models = {\n", "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n", "    \"XGBoost\": XGBRegressor(\n", "        objective=\"reg:squarederror\",\n", "        random_state=42,\n", "    ),\n", "    \"Random Forest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n", "    \"Extra Trees\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n", "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n", "    \"SVR (RBF)\": SVR(kernel=\"rbf\"),\n", "}"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["param_grids = {\n", "    \"Gradient Boosting\": {\n", "        \"model__n_estimators\": [100, 200],\n", "        \"model__learning_rate\": [0.1, 0.05],\n", "        \"model__max_depth\": [3, 5],\n", "        \"model__subsample\": [1.0, 0.8],\n", "    },\n", "    \"XGBoost\": {\n", "        \"model__n_estimators\": [200, 400],\n", "        \"model__learning_rate\": [0.1, 0.05],\n", "        \"model__max_depth\": [4, 6],\n", "        \"model__subsample\": [1.0, 0.8],\n", "        \"model__colsample_bytree\": [1.0, 0.8],\n", "    },\n", "    \"Random Forest\": {\n", "        \"model__n_estimators\": [200, 400],\n", "        \"model__max_depth\": [None, 10, 20],\n", "        \"model__max_features\": [\"sqrt\", \"log2\"],\n", "        \"model__min_samples_split\": [2, 5],\n", "        \"model__min_samples_leaf\": [1, 2],\n", "    },\n", "    \"Extra Trees\": {\n", "        \"model__n_estimators\": [200, 400],\n", "        \"model__max_depth\": [None, 10, 20],\n", "        \"model__max_features\": [\"sqrt\", \"log2\"],\n", "        \"model__min_samples_split\": [2, 5],\n", "        \"model__min_samples_leaf\": [1, 2],\n", "    },\n", "    \"Decision Tree\": {\n", "        \"model__max_depth\": [None, 5, 10, 20],\n", "        \"model__min_samples_split\": [2, 5, 10],\n", "        \"model__min_samples_leaf\": [1, 2, 4],\n", "    },\n", "    \"SVR (RBF)\": {\n", "        \"model__C\": [10, 100],\n", "        \"model__gamma\": [\"scale\", 0.1],\n", "        \"model__epsilon\": [0.1, 0.01],\n", "    },\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Baseline cross\u2011validation comparison"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["cv_results = cross_validate_models(models, preprocessing, X, y, cv=5)\n", "cv_results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Hyperparameter tuning with Grid Search"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["grid_results, best_models = grid_search_all_models(\n", "    models=models,\n", "    param_grids=param_grids,\n", "    preprocessing=preprocessing,\n", "    X_train=X_train,\n", "    y_train=y_train,\n", "    cv=3,\n", ")\n", "grid_results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Final evaluation on the test set"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["test_results = evaluate_best_models(best_models, X_test, y_test)\n", "test_results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Persist the final model"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import joblib\n", "\n", "best_model_name = test_results.iloc[0][\"Model\"]\n", "best_pipeline = best_models[best_model_name]\n", "joblib.dump(best_pipeline, f\"best_model_{best_model_name.replace(' ', '_').lower()}.joblib\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}